## Introduction

Large code models (LCMs) have remarkably advanced the field of code intelligence. Despite their impressive capabilities, they still face practical employment challenges, such as high costs, limited accessibility of proprietary LCMs, and adaptability issues of ultra-large LCMs. These challenges highlight the critical need for more accessible, lightweight yet effective LCMs. In this paper, we propose CycliKD, a Cyclical Knowledge Distillation framework, which aims at continually transferring the programming capabilities of larger, advanced LCMs (Teacher) to smaller, less powerful LCMs (Student). 

CycliKD consists of three stages in one cycle: (1) Correct-and-Fault Knowledge Delivery stage aims at improving the student modelâ€™s capability to recognize errors while ensuring its basic pro- gramming skill during the knowledge transferring, which involves correctness-aware supervised learning and fault-aware contrastive learning methods. (2) Multi-view Feedback stage aims at measuring the quality of results generated by the student model from two views, including model-based and static tool-based measurement; (3) Feedback-based Knowledge Update stage aims at updating the student model adaptively by generating new questions at different difficulty levels, in which the difficulty levels are categorized based on the feedback in the last stage. By performing the training cycle iteratively, the student model is continuously refined through learning more advanced programming skills from the teacher model. Finally, based on the proposed CycliKD framework, we develop a lightweight yet effective LCM, named cycliCoder, which is built upon CodeLlama-7B. 

We validate the effectiveness of CycliCoder across seven programming languages in the code generation task, comparing it with 13 LCMs of various sizes. Experimental results show that CycliCoder achieves a Pass@1 score of 65.2 on the HumanEval benchmark, outperforming over-30B-sized LCMs by an average of 47.51% and surpassing comparable-sized LCMs by an average of 118.47%.

## Dataset

- The correct and faulty data used in Stage I is in [pairwise data](https://github.com/yujiachen99/CycliCoder/blob/main/datas/pairwise_data.json).
- The training data of Analyst model in Stage II is  in [scoring data](https://github.com/yujiachen99/CycliCoder/blob/main/datas/scoring_data.json).
- The generated new data in Stage III is in [new data](https://github.com/yujiachen99/CycliCoder/blob/main/datas/new_data.json).

## Model

We use the [LLaMA-Factory library](https://github.com/hiyouga/LLaMA-Factory) for trainging and inference process. 

####  *We release partial data currently. We will release all code, data, and models (i.e. CycliCoder-7B ) at the time of publication.*

## Citation

@article{zheng2024llamafactory,
  title={LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models},
  author={Yaowei Zheng and Richong Zhang and Junhao Zhang and Yanhan Ye and Zheyan Luo and Yongqiang Ma},
  journal={arXiv preprint arXiv:2403.13372},
  year={2024},
  url={http://arxiv.org/abs/2403.13372}
}


